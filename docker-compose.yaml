services:
  bolt-ai:
    image: bolt-ai:production
    build:
      context: .
      dockerfile: Dockerfile
      target: bolt-ai-production
    ports:
      - "5173:5173"
    env_file: ".env.local"
    environment:
      - NODE_ENV=production
      - COMPOSE_PROFILES=production
      # No strictly neded but serving as hints for Coolify
      - PORT=5173
      - GROQ_API_KEY=${GROQ_API_KEY}
      - HuggingFace_API_KEY=${HuggingFace_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - OPEN_ROUTER_API_KEY=${OPEN_ROUTER_API_KEY}
      - GOOGLE_GENERATIVE_AI_API_KEY=${GOOGLE_GENERATIVE_AI_API_KEY}
      - OLLAMA_API_BASE_URL=${OLLAMA_API_BASE_URL}
      - VITE_LOG_LEVEL=${VITE_LOG_LEVEL:-debug}
      - DEFAULT_NUM_CTX=${DEFAULT_NUM_CTX:-32768}
      - RUNNING_IN_DOCKER=true
    extra_hosts:
      - "host.docker.internal:host-gateway"      
    command: pnpm run dockerstart
    profiles:
      - production  # This service only runs in the production profile

  llamacpp-server:
    image: ghcr.io/ggerganov/llama.cpp:server-cuda #For cpu-only : ghcr.io/ggerganov/llama.cpp:server
    ports:
      - 8080:8080
    volumes:
      - ./models:/models
    environment:
      # alternatively, you can use "LLAMA_ARG_MODEL_URL" to download the model
      LLAMA_ARG_MODEL: /models/qwen2.5-coder-7b-instruct-q4_k_m.gguf
      LLAMA_ARG_CTX_SIZE: 32768
      LLAMA_API_KEY: APIKEYFORLOCALLLAMACPP
      LLAMA_ARG_N_GPU_LAYERS: 999
      LLAMA_ARG_PORT: 8080
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all # alternatively, use `count: all` for all GPUs
              capabilities: [gpu]
    profiles: ["development", "default"]  # This service only runs in the production profile
  bolt-ai-dev:
    image: bolt-ai:development
    build:
      context: .
      dockerfile: Dockerfile
      target: bolt-ai-development
    environment:
      - NODE_ENV=development
      - VITE_HMR_PROTOCOL=ws
      - VITE_HMR_HOST=localhost
      - VITE_HMR_PORT=5173
      - CHOKIDAR_USEPOLLING=true
      - WATCHPACK_POLLING=true  
      - PORT=5173
      - GROQ_API_KEY=${GROQ_API_KEY}
      - HuggingFace_API_KEY=${HuggingFace_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - OPEN_ROUTER_API_KEY=${OPEN_ROUTER_API_KEY}
      - GOOGLE_GENERATIVE_AI_API_KEY=${GOOGLE_GENERATIVE_AI_API_KEY}
      - OLLAMA_API_BASE_URL=${OLLAMA_API_BASE_URL}
      - VITE_LOG_LEVEL=${VITE_LOG_LEVEL:-debug}
      - DEFAULT_NUM_CTX=${DEFAULT_NUM_CTX:-32768}
      - OPENAI_LIKE_API_BASE_URL=http://host.docker.internal:8080
      - OPENAI_LIKE_API_KEY=APIKEYFORLOCALLLAMACPP
      - RUNNING_IN_DOCKER=true
    extra_hosts:
      - "host.docker.internal:host-gateway"      
    volumes:
      - type: bind
        source: .
        target: /app
        consistency: cached
      - /app/node_modules
    ports:
      - "5173:5173"  # Same port, no conflict as only one runs at a time
    command: pnpm run dev --host 0.0.0.0
    profiles: ["development", "default"]  # Make development the default profile
