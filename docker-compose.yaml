services:
  ollama-server:
    image: ollama/ollama
    ports:
      - 11434:11434
    volumes:
      - ./ollama-data:/root/.ollama:rw
      - ./ollama:/usr/local/ollama
    entrypoint: ["/bin/bash", "-c", "/usr/local/ollama/start-ollama.sh"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  bolt-ai-dev:
    image: bolt-ai:development
    build:
      context: .
      dockerfile: Dockerfile
      target: bolt-ai-development
    environment:
      - NODE_ENV=development
      - VITE_HMR_PROTOCOL=ws
      - VITE_HMR_HOST=localhost
      - VITE_HMR_PORT=5173
      - CHOKIDAR_USEPOLLING=true
      - WATCHPACK_POLLING=true  
      - PORT=5173
      - GROQ_API_KEY=${GROQ_API_KEY}
      - HuggingFace_API_KEY=${HuggingFace_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - OPEN_ROUTER_API_KEY=${OPEN_ROUTER_API_KEY}
      - GOOGLE_GENERATIVE_AI_API_KEY=${GOOGLE_GENERATIVE_AI_API_KEY}
      - OLLAMA_API_BASE_URL=http://localhost:11434
      - VITE_LOG_LEVEL=${VITE_LOG_LEVEL:-debug}
      - DEFAULT_NUM_CTX=${DEFAULT_NUM_CTX:-32768}
      - RUNNING_IN_DOCKER=true
    extra_hosts:
      - "host.docker.internal:host-gateway"      
    volumes:
      - type: bind
        source: .
        target: /app
        consistency: cached
      - /app/node_modules
    ports:
      - "5173:5173"  # Same port, no conflict as only one runs at a time
    command: pnpm run dev --host 0.0.0.0
